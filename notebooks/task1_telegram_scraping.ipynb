{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Task 1 — Telegram Scraping\n",
                "\n",
                "This notebook imports and runs the existing Telegram scraper at `src/scraper.py`.\n",
                "\n",
                "Outputs written by the scraper:\n",
                "- data/raw/telegram_messages/YYYY-MM-DD/<channel>.json\n",
                "- data/raw/images/<channel>/* (if enabled)\n",
                "- data/raw/csv/YYYY-MM-DD/telegram_data.csv\n",
                "- data/raw/scrape_state.json\n",
                "- logs/scrape_YYYY-MM-DD.log"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 1,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Environment & Imports\n",
                "from __future__ import annotations\n",
                "\n",
                "import sys\n",
                "from pathlib import Path"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 2,
            "id": "9d1ac804",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "NOTEBOOK_CWD: D:\\Python\\Week 8\\Shipping-a-Data-Product\\notebooks\n",
                        "PROJECT_ROOT: D:\\Python\\Week 8\\Shipping-a-Data-Product\n",
                        "sys.path[0]: D:\\Python\\Week 8\\Shipping-a-Data-Product\n",
                        "Python: d:\\Python\\Week 8\\Shipping-a-Data-Product\\.venv\\Scripts\\python.exe\n"
                    ]
                }
            ],
            "source": [
                "\n",
                "# We are in: <project_root>/notebooks/task1_telegram_scraping.ipynb\n",
                "# So we search upwards to find <project_root>/src/scraper.py\n",
                "NOTEBOOK_CWD: Path = Path.cwd().resolve()\n",
                "\n",
                "\n",
                "def find_project_root(start: Path) -> Path:\n",
                "    for p in [start, *start.parents]:\n",
                "        if (p / \"src\" / \"scraper.py\").exists():\n",
                "            return p\n",
                "    raise FileNotFoundError(\n",
                "        \"Could not locate project root containing src/scraper.py. \"\n",
                "        \"Open this notebook from somewhere inside the repo.\"\n",
                "    )\n",
                "\n",
                "\n",
                "PROJECT_ROOT: Path = find_project_root(NOTEBOOK_CWD)\n",
                "\n",
                "# Make `src` importable as a package\n",
                "if str(PROJECT_ROOT) not in sys.path:\n",
                "    sys.path.insert(0, str(PROJECT_ROOT))\n",
                "\n",
                "print(\"NOTEBOOK_CWD:\", NOTEBOOK_CWD)\n",
                "print(\"PROJECT_ROOT:\", PROJECT_ROOT)\n",
                "print(\"sys.path[0]:\", sys.path[0])\n",
                "print(\"Python:\", sys.executable)\n",
                "\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 3,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Imported scraper from: D:\\Python\\Week 8\\Shipping-a-Data-Product\\src\\scraper.py\n",
                        "MAX_MESSAGES_PER_CHANNEL = 300\n",
                        "LOOKBACK_DAYS = 14\n",
                        "DOWNLOAD_MEDIA = True\n",
                        "MAX_MEDIA_PER_CHANNEL = 50\n",
                        "PER_CHANNEL_TIMEOUT_SEC = 180\n",
                        "STATE_PATH = D:\\Python\\Week 8\\Shipping-a-Data-Product\\data\\raw\\scrape_state.json\n",
                        "DATA_LAKE_DIR = D:\\Python\\Week 8\\Shipping-a-Data-Product\\data\\raw\\telegram_messages\n",
                        "IMAGES_DIR = D:\\Python\\Week 8\\Shipping-a-Data-Product\\data\\raw\\images\n",
                        "CSV_DIR = D:\\Python\\Week 8\\Shipping-a-Data-Product\\data\\raw\\csv\n"
                    ]
                }
            ],
            "source": [
                "# Import scraper and show config\n",
                "from src import scraper\n",
                "\n",
                "print(\"Imported scraper from:\", scraper.__file__)\n",
                "\n",
                "# Show important config (read at import time from env vars)\n",
                "for name in [\n",
                "    \"MAX_MESSAGES_PER_CHANNEL\",\n",
                "    \"LOOKBACK_DAYS\",\n",
                "    \"DOWNLOAD_MEDIA\",\n",
                "    \"MAX_MEDIA_PER_CHANNEL\",\n",
                "    \"PER_CHANNEL_TIMEOUT_SEC\",\n",
                "    \"STATE_PATH\",\n",
                "    \"DATA_LAKE_DIR\",\n",
                "    \"IMAGES_DIR\",\n",
                "    \"CSV_DIR\",\n",
                "]:\n",
                "    print(f\"{name} = {getattr(scraper, name, None)}\")\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Run the scraper\n",
                "\n",
                "In Jupyter, run the async entrypoint with:\n",
                "Do **not** call `python src/scraper.py` from inside the notebook.\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 4,
            "metadata": {},
            "outputs": [
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "2026-01-15 19:04:14,765 | INFO | === Starting Telegram scraping ===\n",
                        "2026-01-15 19:04:14,769 | INFO | Connecting to 149.154.167.91:443/TcpFull...\n",
                        "2026-01-15 19:04:14,916 | INFO | Connection to 149.154.167.91:443/TcpFull complete!\n",
                        "2026-01-15 19:04:16,074 | INFO | Scraping @CheMed123 (last_id=97)\n",
                        "2026-01-15 19:04:16,565 | INFO | Scraping @lobelia4cosmetics (last_id=22887)\n",
                        "2026-01-15 19:04:17,013 | INFO | Scraping @rayapharmaceuticals (last_id=0)\n",
                        "2026-01-15 19:04:17,470 | INFO | Scraping @tikvahpharma (last_id=188947)\n",
                        "2026-01-15 19:04:18,281 | INFO | Disconnecting from 149.154.167.91:443/TcpFull...\n",
                        "2026-01-15 19:04:18,282 | INFO | Disconnection from 149.154.167.91:443/TcpFull complete!\n",
                        "2026-01-15 19:04:18,311 | INFO | === Scraping complete ===\n"
                    ]
                }
            ],
            "source": [
                "await scraper.main()\n"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "2fe7369b",
            "metadata": {},
            "source": [
                "## Verification — State File"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 5,
            "id": "6495a658",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "State exists: True -> D:\\Python\\Week 8\\Shipping-a-Data-Product\\data\\raw\\scrape_state.json\n",
                        "\n",
                        "Channels in state:\n",
                        " CheMed123: {'last_message_id': 97, 'updated_at': '2026-01-14T18:08:14.968611+00:00'}\n",
                        " lobelia4cosmetics: 22887\n",
                        " tikvahpharma: 188947\n"
                    ]
                }
            ],
            "source": [
                "import json\n",
                "\n",
                "\n",
                "state_path = PROJECT_ROOT / \"data\" / \"raw\" / \"scrape_state.json\"\n",
                "\n",
                "\n",
                "print(\"State exists:\", state_path.exists(), \"->\", state_path)\n",
                "\n",
                "\n",
                "if state_path.exists():\n",
                "\tstate = json.loads(state_path.read_text(encoding=\"utf-8\"))\n",
                "\tprint(\"\\nChannels in state:\")\n",
                "\tfor ch, v in (state.get(\"channels\") or {}).items():\n",
                "\t\tprint(f\" {ch}: {v}\")\n",
                "else:\n",
                "\tprint(\"No state file found yet. Run the scraper cell first.\")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "d0a45284",
            "metadata": {},
            "source": [
                "## Verification — JSON Partitions"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 6,
            "id": "92626771",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Partitions dir exists: True -> D:\\Python\\Week 8\\Shipping-a-Data-Product\\data\\raw\\telegram_messages\n",
                        "\n",
                        "Newest day partitions:\n",
                        " 2026-01-15: 2 json files\n",
                        " - lobelia4cosmetics.json\n",
                        " - tikvahpharma.json\n",
                        " 2026-01-14: 2 json files\n",
                        " - lobelia4cosmetics.json\n",
                        " - tikvahpharma.json\n",
                        " 2026-01-13: 2 json files\n",
                        " - lobelia4cosmetics.json\n",
                        " - tikvahpharma.json\n",
                        " 2026-01-12: 2 json files\n",
                        " - lobelia4cosmetics.json\n",
                        " - tikvahpharma.json\n",
                        " 2026-01-11: 2 json files\n",
                        " - lobelia4cosmetics.json\n",
                        " - tikvahpharma.json\n",
                        " 2026-01-10: 2 json files\n",
                        " - lobelia4cosmetics.json\n",
                        " - tikvahpharma.json\n",
                        " 2026-01-09: 2 json files\n",
                        " - lobelia4cosmetics.json\n",
                        " - tikvahpharma.json\n"
                    ]
                }
            ],
            "source": [
                "data_dir = PROJECT_ROOT / \"data\" / \"raw\" / \"telegram_messages\"\n",
                "\n",
                "\n",
                "print(\"Partitions dir exists:\", data_dir.exists(), \"->\", data_dir)\n",
                "\n",
                "\n",
                "if data_dir.exists():\n",
                "\tdays = sorted([p for p in data_dir.iterdir() if p.is_dir()], reverse=True)\n",
                "\tprint(\"\\nNewest day partitions:\")\n",
                "\tfor d in days[:7]:\n",
                "\t\tfiles = sorted(d.glob(\"*.json\"))\n",
                "\t\tprint(f\" {d.name}: {len(files)} json files\")\n",
                "\t\tfor f in files[:5]:\n",
                "\t\t\tprint(\" -\", f.name)\n",
                "else:\n",
                "\tprint(\"No partitions found yet. Run the scraper cell first.\")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "c1d40b6e",
            "metadata": {},
            "source": [
                "## Verification — CSV Backups"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 7,
            "id": "a9870a56",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "CSV dir exists: True -> D:\\Python\\Week 8\\Shipping-a-Data-Product\\data\\raw\\csv\n",
                        "\n",
                        "Newest CSV backups:\n",
                        " 2026-01-15: telegram_data.csv (1668 bytes)\n"
                    ]
                }
            ],
            "source": [
                "csv_dir = PROJECT_ROOT / \"data\" / \"raw\" / \"csv\"\n",
                "\n",
                "print(\"CSV dir exists:\", csv_dir.exists(), \"->\", csv_dir)\n",
                "\n",
                "if csv_dir.exists():\n",
                "\tdays = sorted([p for p in csv_dir.iterdir() if p.is_dir()], reverse=True)\n",
                "\tprint(\"\\nNewest CSV backups:\")\n",
                "\tfor d in days[:7]:\n",
                "\t\tcsv_file = d / \"telegram_data.csv\"\n",
                "\t\tif csv_file.exists():\n",
                "\t\t\tprint(f\" {d.name}: telegram_data.csv ({csv_file.stat().st_size} bytes)\")\n",
                "\t\telse:\n",
                "\t\t\tprint(f\" {d.name}: telegram_data.csv MISSING\")\n",
                "else:\n",
                "\tprint(\"No CSV backups found yet. Run the scraper cell first.\")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "1ff4b323",
            "metadata": {},
            "source": [
                "## Verification — Image Downloads "
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 8,
            "id": "8cab1f88",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Images dir exists: True -> D:\\Python\\Week 8\\Shipping-a-Data-Product\\data\\raw\\images\n",
                        "\n",
                        "Image folders:\n",
                        " CheMed123: 69 files\n",
                        " lobelia4cosmetics: 2513 files\n",
                        " tikvahpharma: 5589 files\n"
                    ]
                }
            ],
            "source": [
                "images_dir = PROJECT_ROOT / \"data\" / \"raw\" / \"images\"\n",
                "\n",
                "\n",
                "print(\"Images dir exists:\", images_dir.exists(), \"->\", images_dir)\n",
                "\n",
                "\n",
                "if images_dir.exists():\n",
                "\tchans = sorted([p for p in images_dir.iterdir() if p.is_dir()])\n",
                "\tprint(\"\\nImage folders:\")\n",
                "\tfor ch in chans:\n",
                "\t\tn = len(list(ch.glob(\"*\")))\n",
                "\t\tprint(f\" {ch.name}: {n} files\")\n",
                "else:\n",
                "\tprint(\"No images found yet. Run the scraper cell first (and enable media download).\")"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": ".venv",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.13.9"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}
