{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Task 1 â€” Telegram Scraping\n",
                "\n",
                "This notebook imports and runs the existing Telegram scraper at `src/scraper.py`.\n",
                "\n",
                "Outputs written by the scraper:\n",
                "- `data/raw/telegram_messages/YYYY-MM-DD/<channel>.json`\n",
                "- `data/raw/images/<channel>/*` (if enabled)\n",
                "- `data/raw/scrape_state.json`\n",
                "- `logs/scraper.log`\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 1,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "NOTEBOOK_CWD: D:\\Python\\Week 8\\Shipping-a-Data-Product\\notebooks\n",
                        "PROJECT_ROOT: D:\\Python\\Week 8\\Shipping-a-Data-Product\n",
                        "sys.path[0]: D:\\Python\\Week 8\\Shipping-a-Data-Product\n",
                        "Python: d:\\Python\\Week 8\\Shipping-a-Data-Product\\.venv\\Scripts\\python.exe\n"
                    ]
                }
            ],
            "source": [
                "from __future__ import annotations\n",
                "\n",
                "import sys\n",
                "from pathlib import Path\n",
                "\n",
                "# We are in: <project_root>/notebooks/task1_telegram_scraping.ipynb\n",
                "# So we search upwards to find <project_root>/src/scraper.py\n",
                "NOTEBOOK_CWD: Path = Path.cwd().resolve()\n",
                "\n",
                "\n",
                "def find_project_root(start: Path) -> Path:\n",
                "    for p in [start, *start.parents]:\n",
                "        if (p / \"src\" / \"scraper.py\").exists():\n",
                "            return p\n",
                "    raise FileNotFoundError(\n",
                "        \"Could not locate project root containing src/scraper.py. \"\n",
                "        \"Open this notebook from somewhere inside the repo.\"\n",
                "    )\n",
                "\n",
                "\n",
                "PROJECT_ROOT: Path = find_project_root(NOTEBOOK_CWD)\n",
                "\n",
                "# Make `src` importable as a package\n",
                "if str(PROJECT_ROOT) not in sys.path:\n",
                "    sys.path.insert(0, str(PROJECT_ROOT))\n",
                "\n",
                "print(\"NOTEBOOK_CWD:\", NOTEBOOK_CWD)\n",
                "print(\"PROJECT_ROOT:\", PROJECT_ROOT)\n",
                "print(\"sys.path[0]:\", sys.path[0])\n",
                "print(\"Python:\", sys.executable)\n",
                "\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 2,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Imported scraper from: D:\\Python\\Week 8\\Shipping-a-Data-Product\\src\\scraper.py\n",
                        "MAX_MESSAGES_PER_CHANNEL = 300\n",
                        "LOOKBACK_DAYS = 14\n",
                        "DOWNLOAD_MEDIA = True\n",
                        "MAX_MEDIA_PER_CHANNEL = 50\n",
                        "PER_CHANNEL_TIMEOUT_SEC = 180\n",
                        "STATE_PATH = D:\\Python\\Week 8\\Shipping-a-Data-Product\\data\\raw\\scrape_state.json\n",
                        "DATA_LAKE_DIR = D:\\Python\\Week 8\\Shipping-a-Data-Product\\data\\raw\\telegram_messages\n",
                        "IMAGES_DIR = D:\\Python\\Week 8\\Shipping-a-Data-Product\\data\\raw\\images\n"
                    ]
                }
            ],
            "source": [
                "from src import scraper\n",
                "\n",
                "print(\"Imported scraper from:\", scraper.__file__)\n",
                "\n",
                "# Show important config (read at import time from env vars)\n",
                "for name in [\n",
                "    \"MAX_MESSAGES_PER_CHANNEL\",\n",
                "    \"LOOKBACK_DAYS\",\n",
                "    \"DOWNLOAD_MEDIA\",\n",
                "    \"MAX_MEDIA_PER_CHANNEL\",\n",
                "    \"PER_CHANNEL_TIMEOUT_SEC\",\n",
                "    \"STATE_PATH\",\n",
                "    \"DATA_LAKE_DIR\",\n",
                "    \"IMAGES_DIR\",\n",
                "]:\n",
                "    print(f\"{name} = {getattr(scraper, name, None)}\")\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Run the scraper\n",
                "\n",
                "In Jupyter, run the async entrypoint with:\n",
                "\n",
                "```python\n",
                "await scraper.main()\n",
                "```\n",
                "\n",
                "Do **not** call `python src/scraper.py` from inside the notebook.\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 3,
            "metadata": {},
            "outputs": [
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "2026-01-15 17:55:34,512 | INFO | === Telegram scraper starting ===\n",
                        "2026-01-15 17:55:34,514 | INFO | PROJECT_ROOT=D:\\Python\\Week 8\\Shipping-a-Data-Product\n",
                        "2026-01-15 17:55:34,515 | INFO | STATE_PATH=D:\\Python\\Week 8\\Shipping-a-Data-Product\\data\\raw\\scrape_state.json\n",
                        "2026-01-15 17:55:34,516 | INFO | DATA_LAKE_DIR=D:\\Python\\Week 8\\Shipping-a-Data-Product\\data\\raw\\telegram_messages\n",
                        "2026-01-15 17:55:34,516 | INFO | IMAGES_DIR=D:\\Python\\Week 8\\Shipping-a-Data-Product\\data\\raw\\images\n",
                        "2026-01-15 17:55:34,517 | INFO | CONFIG: MAX_MESSAGES_PER_CHANNEL=300, LOOKBACK_DAYS=14, DOWNLOAD_MEDIA=True, MAX_MEDIA_PER_CHANNEL=50, PER_CHANNEL_TIMEOUT_SEC=180\n",
                        "2026-01-15 17:55:34,518 | INFO | Loaded additional channels from D:\\Python\\Week 8\\Shipping-a-Data-Product\\channels.txt\n",
                        "2026-01-15 17:55:34,519 | INFO | Total channels to scrape: 4 -> @CheMed123, @lobelia4cosmetics, @tikvahpharma, @rayapharmaceuticals\n",
                        "2026-01-15 17:55:34,550 | INFO | Connecting to 149.154.167.51:443/TcpFull...\n",
                        "2026-01-15 17:55:36,063 | INFO | Connection to 149.154.167.51:443/TcpFull complete!\n",
                        "2026-01-15 17:55:52,622 | INFO | Phone migrated to 4\n",
                        "2026-01-15 17:55:52,788 | INFO | Reconnecting to new data center 4\n",
                        "2026-01-15 17:55:52,962 | INFO | Disconnecting from 149.154.167.51:443/TcpFull...\n",
                        "2026-01-15 17:55:52,964 | INFO | Disconnection from 149.154.167.51:443/TcpFull complete!\n",
                        "2026-01-15 17:55:52,964 | INFO | Connecting to 149.154.167.91:443/TcpFull...\n",
                        "2026-01-15 17:55:54,725 | INFO | Connection to 149.154.167.91:443/TcpFull complete!\n",
                        "2026-01-15 17:56:02,412 | INFO | Telegram client connected.\n",
                        "2026-01-15 17:56:02,413 | INFO | Scraping channel: @CheMed123\n",
                        "2026-01-15 17:56:02,413 | INFO | Channel @CheMed123 last_message_id=97\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Signed in successfully as MogassaðŸ‡ªðŸ‡¹; remember to not break the ToS or you will risk an account ban!\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "2026-01-15 17:56:03,211 | INFO | Channel @CheMed123 done: new_messages=0, images_downloaded=0, partitions=0, new_last_message_id=97\n",
                        "2026-01-15 17:56:03,212 | INFO | Scraping channel: @lobelia4cosmetics\n",
                        "2026-01-15 17:56:03,212 | INFO | Channel @lobelia4cosmetics last_message_id=22884\n",
                        "2026-01-15 17:56:03,773 | INFO | Channel @lobelia4cosmetics done: new_messages=0, images_downloaded=0, partitions=0, new_last_message_id=22884\n",
                        "2026-01-15 17:56:03,774 | INFO | Scraping channel: @tikvahpharma\n",
                        "2026-01-15 17:56:03,774 | INFO | Channel @tikvahpharma last_message_id=188947\n",
                        "2026-01-15 17:56:04,578 | INFO | Channel @tikvahpharma done: new_messages=0, images_downloaded=0, partitions=0, new_last_message_id=188947\n",
                        "2026-01-15 17:56:04,579 | INFO | Scraping channel: @rayapharmaceuticals\n",
                        "2026-01-15 17:56:04,579 | INFO | Channel @rayapharmaceuticals last_message_id=0\n",
                        "2026-01-15 17:56:05,268 | INFO | Channel @rayapharmaceuticals done: new_messages=0, images_downloaded=0, partitions=0, new_last_message_id=0\n",
                        "2026-01-15 17:56:05,270 | INFO | Disconnecting from 149.154.167.91:443/TcpFull...\n",
                        "2026-01-15 17:56:05,271 | INFO | Disconnection from 149.154.167.91:443/TcpFull complete!\n",
                        "2026-01-15 17:56:05,277 | INFO | === Telegram scraper finished ===\n"
                    ]
                }
            ],
            "source": [
                "await scraper.main()\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Quick verification (outputs)\n",
                "\n",
                "This cell checks the state file and lists recent partitions and image folders.\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 4,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "State exists: True -> D:\\Python\\Week 8\\Shipping-a-Data-Product\\data\\raw\\scrape_state.json\n",
                        "Partitions dir exists: True -> D:\\Python\\Week 8\\Shipping-a-Data-Product\\data\\raw\\telegram_messages\n",
                        "Images dir exists: True -> D:\\Python\\Week 8\\Shipping-a-Data-Product\\data\\raw\\images\n",
                        "\n",
                        "State (channels):\n",
                        "  CheMed123: {'last_message_id': 97, 'updated_at': '2026-01-14T18:08:14.968611+00:00'}\n",
                        "  lobelia4cosmetics: {'last_message_id': 22884, 'updated_at': '2026-01-15T13:20:06.955975+00:00'}\n",
                        "  tikvahpharma: 188947\n",
                        "\n",
                        "Newest day partitions:\n",
                        "  2026-01-15: 2 json files\n",
                        "    - lobelia4cosmetics.json\n",
                        "    - tikvahpharma.json\n",
                        "  2026-01-14: 2 json files\n",
                        "    - lobelia4cosmetics.json\n",
                        "    - tikvahpharma.json\n",
                        "  2026-01-13: 2 json files\n",
                        "    - lobelia4cosmetics.json\n",
                        "    - tikvahpharma.json\n",
                        "  2026-01-12: 2 json files\n",
                        "    - lobelia4cosmetics.json\n",
                        "    - tikvahpharma.json\n",
                        "  2026-01-11: 2 json files\n",
                        "    - lobelia4cosmetics.json\n",
                        "    - tikvahpharma.json\n",
                        "  2026-01-10: 2 json files\n",
                        "    - lobelia4cosmetics.json\n",
                        "    - tikvahpharma.json\n",
                        "  2026-01-09: 2 json files\n",
                        "    - lobelia4cosmetics.json\n",
                        "    - tikvahpharma.json\n",
                        "\n",
                        "Image folders:\n",
                        "  CheMed123: 69 files\n",
                        "  lobelia4cosmetics: 2513 files\n",
                        "  tikvahpharma: 5589 files\n"
                    ]
                }
            ],
            "source": [
                "import json\n",
                "from pathlib import Path\n",
                "\n",
                "state_path = PROJECT_ROOT / \"data\" / \"raw\" / \"scrape_state.json\"\n",
                "data_dir = PROJECT_ROOT / \"data\" / \"raw\" / \"telegram_messages\"\n",
                "images_dir = PROJECT_ROOT / \"data\" / \"raw\" / \"images\"\n",
                "\n",
                "print(\"State exists:\", state_path.exists(), \"->\", state_path)\n",
                "print(\"Partitions dir exists:\", data_dir.exists(), \"->\", data_dir)\n",
                "print(\"Images dir exists:\", images_dir.exists(), \"->\", images_dir)\n",
                "\n",
                "if state_path.exists():\n",
                "    st = json.loads(state_path.read_text(encoding=\"utf-8\"))\n",
                "    print(\"\\nState (channels):\")\n",
                "    for ch, v in (st.get(\"channels\") or {}).items():\n",
                "        print(f\"  {ch}: {v}\")\n",
                "\n",
                "if data_dir.exists():\n",
                "    days = sorted([p for p in data_dir.iterdir() if p.is_dir()], reverse=True)\n",
                "    print(\"\\nNewest day partitions:\")\n",
                "    for d in days[:7]:\n",
                "        files = sorted(d.glob(\"*.json\"))\n",
                "        print(f\"  {d.name}: {len(files)} json files\")\n",
                "        for f in files[:5]:\n",
                "            print(\"    -\", f.name)\n",
                "\n",
                "if images_dir.exists():\n",
                "    chans = sorted([p for p in images_dir.iterdir() if p.is_dir()])\n",
                "    print(\"\\nImage folders:\")\n",
                "    for ch in chans:\n",
                "        n = len(list(ch.glob(\"*\")))\n",
                "        print(f\"  {ch.name}: {n} files\")\n"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": ".venv",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.13.9"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}
