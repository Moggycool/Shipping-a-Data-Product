{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Task 02 — Data Modeling & Transformation (dbt + Postgres)\n",
                "\n",
                "**Project workspace**\n",
                "- dbt project root: `D:/Python/Week 8/Shipping-a-Data-Product/medical_warehouse`\n",
                "- models:\n",
                "  - staging: `medical_warehouse/models/staging/stg_telegram_messages.sql`\n",
                "  - marts: `medical_warehouse/models/marts/dim_channels.sql`, `dim_dates.sql`, `fct_messages.sql`\n",
                "- schema docs/tests:\n",
                "  - `medical_warehouse/models/staging/schema.yml`\n",
                "  - `medical_warehouse/models/marts/schema.yml`\n",
                "- singular tests: `medical_warehouse/tests/`\n",
                "- upstream scripts (reference): `src/scraper.py`, `src/load_raw_to_postgres.py`, `src/login_test.py`\n",
                "\n",
                "## What this notebook demonstrates (Task 2 requirements)\n",
                "1. dbt project runs successfully (deps/debug/build/test)\n",
                "2. Transformation layer implemented: staging + dimensional marts (star schema)\n",
                "3. Data quality tests executed and passing\n",
                "4. Documentation artifacts generated (manifest/catalog)\n",
                "5. Direct Postgres SQL validation: relation existence, row counts, referential integrity\n",
                "\n",
                "> Important: If your Postgres shows schemas like `analytics_staging` / `analytics_analytics` instead of `staging` / `analytics`, that's normal. This notebook reads `target/manifest.json` to discover the **actual schema names** and queries those.\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 0) Setup notes\n",
                "\n",
                "If dbt cannot find `profiles.yml`, set `DBT_PROFILES_DIR`:\n",
                "\n",
                "```powershell\n",
                "$env:DBT_PROFILES_DIR = 'D:\\\\Python\\\\Week 8\\\\Shipping-a-Data-Product\\\\medical_warehouse'\n",
                "# optionally:\n",
                "$env:PGPASSWORD = 'your_password'\n",
                "```\n",
                "\n",
                "Then restart your notebook kernel.\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from __future__ import annotations\n",
                "\n",
                "import os\n",
                "import json\n",
                "import subprocess\n",
                "from pathlib import Path\n",
                "from dataclasses import dataclass\n",
                "from typing import Any, Dict, Optional, List\n",
                "\n",
                "import pandas as pd\n",
                "\n",
                "try:\n",
                "    import yaml\n",
                "except Exception as e:\n",
                "    raise RuntimeError('Missing dependency: PyYAML. Install with: pip install pyyaml') from e\n",
                "\n",
                "try:\n",
                "    from sqlalchemy import create_engine, text\n",
                "except Exception as e:\n",
                "    raise RuntimeError('Missing dependency: SQLAlchemy (and psycopg2). Install with: pip install sqlalchemy psycopg2-binary') from e\n",
                "\n",
                "# --- Paths (based on your workspace) ---\n",
                "REPO_ROOT = Path(r'D:\\\\Python\\\\Week 8\\\\Shipping-a-Data-Product')\n",
                "PROJECT_DIR = REPO_ROOT / 'medical_warehouse'\n",
                "\n",
                "DBT_PROJECT_YML = PROJECT_DIR / 'dbt_project.yml'\n",
                "PROFILES_YML = PROJECT_DIR / 'profiles.yml'\n",
                "TARGET_DIR = PROJECT_DIR / 'target'\n",
                "MANIFEST_JSON = TARGET_DIR / 'manifest.json'\n",
                "\n",
                "print('REPO_ROOT:', REPO_ROOT)\n",
                "print('PROJECT_DIR:', PROJECT_DIR)\n",
                "print('dbt_project.yml exists:', DBT_PROJECT_YML.exists())\n",
                "print('profiles.yml exists:', PROFILES_YML.exists())\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1) Helper functions\n",
                "These helpers:\n",
                "- run dbt CLI commands\n",
                "- read `profiles.yml` (Postgres connection)\n",
                "- parse `target/manifest.json` to find actual schemas/aliases\n",
                "- run SQL queries and return results as DataFrames\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def run_cmd(cmd: List[str], cwd: Path = PROJECT_DIR, check: bool = True):\n",
                "    \"\"\"Run a command and print stdout/stderr.\"\"\"\n",
                "    print('\\n$ ' + ' '.join(cmd))\n",
                "    proc = subprocess.run(cmd, cwd=str(cwd), text=True, capture_output=True)\n",
                "    if proc.stdout:\n",
                "        print(proc.stdout)\n",
                "    if proc.stderr:\n",
                "        print(proc.stderr)\n",
                "    if check and proc.returncode != 0:\n",
                "        raise RuntimeError(f'Command failed ({proc.returncode}): {cmd}')\n",
                "    return proc\n",
                "\n",
                "def read_yaml(path: Path) -> Dict[str, Any]:\n",
                "    with open(path, 'r', encoding='utf-8') as f:\n",
                "        return yaml.safe_load(f)\n",
                "\n",
                "@dataclass\n",
                "class DbtProfileConn:\n",
                "    host: str\n",
                "    port: int\n",
                "    user: str\n",
                "    password: Optional[str]\n",
                "    dbname: str\n",
                "    schema: str\n",
                "\n",
                "def load_dbt_profile_conn(profiles_yml: Path = PROFILES_YML) -> DbtProfileConn:\n",
                "    \"\"\"Load target connection info from profiles.yml.\n",
                "\n",
                "    Note: This assumes the first top-level profile in profiles.yml is the one used.\n",
                "    If you have multiple profiles, adjust this function to choose by name.\n",
                "    \"\"\"\n",
                "    data = read_yaml(profiles_yml)\n",
                "    if not isinstance(data, dict) or not data:\n",
                "        raise ValueError('profiles.yml is empty/invalid')\n",
                "\n",
                "    profile_name = next(iter(data.keys()))\n",
                "    profile = data[profile_name]\n",
                "    target_name = profile.get('target')\n",
                "    outputs = profile.get('outputs', {})\n",
                "\n",
                "    if not target_name or target_name not in outputs:\n",
                "        raise ValueError(f'Cannot resolve target from profiles.yml (profile={profile_name}, target={target_name})')\n",
                "\n",
                "    out = outputs[target_name]\n",
                "    host = out.get('host', 'localhost')\n",
                "    port = int(out.get('port', 5432))\n",
                "    user = out.get('user')\n",
                "    password = out.get('password') or os.environ.get('PGPASSWORD')\n",
                "    dbname = out.get('dbname') or out.get('database')\n",
                "    schema = out.get('schema', 'public')\n",
                "\n",
                "    if not user or not dbname:\n",
                "        raise ValueError('profiles.yml missing required fields user/dbname')\n",
                "\n",
                "    return DbtProfileConn(host=host, port=port, user=user, password=password, dbname=dbname, schema=schema)\n",
                "\n",
                "def make_engine(conn: DbtProfileConn):\n",
                "    # If password is None, you likely need to set $env:PGPASSWORD or fill profiles.yml\n",
                "    pw = '' if conn.password is None else conn.password\n",
                "    url = f'postgresql+psycopg2://{conn.user}:{pw}@{conn.host}:{conn.port}/{conn.dbname}'\n",
                "    return create_engine(url)\n",
                "\n",
                "def df_sql(engine, sql: str, params: Optional[dict] = None) -> pd.DataFrame:\n",
                "    with engine.connect() as c:\n",
                "        return pd.read_sql(text(sql), c, params=params)\n",
                "\n",
                "def load_manifest_models(manifest_path: Path = MANIFEST_JSON, project_name: str = 'medical_warehouse') -> pd.DataFrame:\n",
                "    \"\"\"Return a DataFrame of project models and the actual schemas/aliases dbt compiled.\"\"\"\n",
                "    if not manifest_path.exists():\n",
                "        raise FileNotFoundError(f'manifest.json not found at {manifest_path}. Run `dbt docs generate` or `dbt build` first.')\n",
                "    manifest = json.loads(manifest_path.read_text(encoding='utf-8'))\n",
                "    rows = []\n",
                "    for unique_id, node in (manifest.get('nodes') or {}).items():\n",
                "        if node.get('resource_type') != 'model':\n",
                "            continue\n",
                "        if node.get('package_name') != project_name:\n",
                "            continue\n",
                "        rows.append({\n",
                "            'unique_id': unique_id,\n",
                "            'name': node.get('name'),\n",
                "            'alias': node.get('alias') or node.get('name'),\n",
                "            'schema': node.get('schema'),\n",
                "            'database': node.get('database'),\n",
                "            'materialized': (node.get('config') or {}).get('materialized'),\n",
                "            'original_file_path': node.get('original_file_path'),\n",
                "        })\n",
                "    return pd.DataFrame(rows).sort_values(['schema', 'name']).reset_index(drop=True)\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2) dbt dependencies + debug (Task 2 evidence)\n",
                "- `dbt deps`: installs packages (e.g., dbt_utils)\n",
                "- `dbt debug`: validates profiles + connection\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "run_cmd(['dbt', 'deps'])\n",
                "run_cmd(['dbt', 'debug'])\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3) Build models + run tests (Task 2 core)\n",
                "`dbt build` runs models and tests in dependency order.\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "run_cmd(['dbt', 'build'])\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 4) Generate dbt docs artifacts\n",
                "Creates `target/manifest.json` (lineage + compiled config) and catalog.\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "run_cmd(['dbt', 'docs', 'generate'])\n",
                "print('manifest exists:', MANIFEST_JSON.exists())\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 5) Inspect actual schemas/relations from the manifest\n",
                "This is the authoritative way to know where dbt built relations.\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "models_df = load_manifest_models()\n",
                "models_df\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 6) Connect to Postgres using profiles.yml\n",
                "This connects to the same DB target dbt uses (assuming you run this from the same profiles).\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "conn = load_dbt_profile_conn()\n",
                "print(conn)\n",
                "engine = make_engine(conn)\n",
                "\n",
                "# List schemas in the connected DB\n",
                "df_sql(engine, \"\"\"\n",
                "select schema_name\n",
                "from information_schema.schemata\n",
                "order by schema_name\n",
                "\"\"\")\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 7) Verify dbt relations exist in the DB\n",
                "We query tables/views in the schemas that the manifest reports.\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "expected_schemas = sorted(set(models_df['schema'].dropna().tolist()))\n",
                "print('Schemas from manifest:', expected_schemas)\n",
                "\n",
                "df_sql(engine, \"\"\"\n",
                "select table_schema, table_name, table_type\n",
                "from information_schema.tables\n",
                "where table_schema = any(:schemas)\n",
                "order by table_schema, table_name\n",
                "\"\"\", params={'schemas': expected_schemas})\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 8) Row counts\n",
                "Shows row counts for staging + marts models.\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "rels = {r['name']: (r['schema'], r['alias']) for _, r in models_df.iterrows()}\n",
                "rels\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from sqlalchemy import text\n",
                "\n",
                "\n",
                "def count_relation(schema: str, relation: str) -> int:\n",
                "    sql = text(f\"select count(*) as n from {schema}.{relation}\")\n",
                "    with engine.connect() as conn:\n",
                "        return int(conn.execute(sql).scalar_one())\n",
                "\n",
                "\n",
                "for model_name, (schema, relname) in rels.items():\n",
                "    try:\n",
                "        n = count_relation(schema, relname)\n",
                "        print(f\"{schema}.{relname}: {n:,}\")\n",
                "    except Exception as e:\n",
                "        print(f\"FAILED counting {schema}.{relname}: {e}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 9) Referential integrity: fact → dimensions\n",
                "These SQL checks complement dbt relationship tests.\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "schema_fct, fct = rels['fct_messages']\n",
                "schema_ch, dim_ch = rels['dim_channels']\n",
                "schema_dt, dim_dt = rels['dim_dates']\n",
                "\n",
                "df_sql(engine, f\"\"\"\n",
                "select count(*) as orphan_channels\n",
                "from {schema_fct}.{fct} f\n",
                "left join {schema_ch}.{dim_ch} d\n",
                "  on f.channel_key = d.channel_key\n",
                "where d.channel_key is null\n",
                "\"\"\")\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "df_sql(engine, f\"\"\"\n",
                "select count(*) as orphan_dates\n",
                "from {schema_fct}.{fct} f\n",
                "left join {schema_dt}.{dim_dt} d\n",
                "  on f.date_key = d.date_key\n",
                "where d.date_key is null\n",
                "\"\"\")\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 10) Business-rule checks (SQL evidence)\n",
                "These are common singular-test style checks.\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "schema_stg, stg = rels['stg_telegram_messages']\n",
                "\n",
                "df_sql(engine, f\"\"\"\n",
                "select count(*) as future_messages\n",
                "from {schema_stg}.{stg}\n",
                "where message_ts > now()\n",
                "\"\"\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "df_sql(engine, f\"\"\"\n",
                "select count(*) as future_messages\n",
                "from {schema_stg}.{stg}\n",
                "where message_ts::date > current_date\n",
                "\"\"\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 11) Run dbt tests explicitly\n",
                "Even if you ran `dbt build`, it’s useful to show `dbt test` output separately for Task 2.\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "run_cmd(['dbt', 'test'])\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 12) Optional: Serve dbt docs (interactive)\n",
                "This starts a local web server to view the DAG.\n",
                "Stop the cell/kernel to end.\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "run_cmd(['dbt', 'docs', 'serve', '--port', '8081'], check=False)\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Task 2 submission checklist\n",
                "For your report/screenshots:\n",
                "1. `dbt debug` success\n",
                "2. `dbt build` success\n",
                "3. `dbt test` success (0 failures)\n",
                "4. `models_df` table showing model → schema mapping\n",
                "5. Postgres evidence: tables/views exist in those schemas\n",
                "6. Row counts for stg/dim/fct\n",
                "7. Orphan checks show 0 orphans\n",
                "8. Optional: dbt docs DAG screenshot\n"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": ".venv",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.13.9"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}
